ASSGN4 A1)
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, SimpleRNN, Dense

# Load shakespeare.txt (only first 250 characters)
with open('/content/sample_data/shakespeare.txt', 'r') as file:
    text = file.read()[:250]

# Preprocess the text
tokenizer = Tokenizer(char_level=True)  # Character-level tokenization
tokenizer.fit_on_texts([text])
total_chars = len(tokenizer.word_index) + 1  # Total unique characters

# Convert the text into a sequence of integers
input_sequences = []
next_chars = []
seq_length = 40  # Let's take 40 characters as input sequence

for i in range(0, len(text) - seq_length):
    input_sequences.append(text[i: i + seq_length])
    next_chars.append(text[i + seq_length])

# Convert input sequences to integer sequences
X = np.array([tokenizer.texts_to_sequences([seq])[0] for seq in input_sequences])
y = np.array([tokenizer.texts_to_sequences([next_char])[0][0] for next_char in next_chars])

# Convert output to categorical (one-hot encoding)
y = to_categorical(y, num_classes=total_chars)

# Define the RNN model
model = Sequential()
model.add(Embedding(input_dim=total_chars, output_dim=64, input_length=seq_length))
model.add(SimpleRNN(128, activation='relu', return_sequences=False))  # RNN with ReLU
model.add(Dense(total_chars, activation='softmax'))  # Softmax for multi-class prediction

# Compile the model
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
# Train the model
model.fit(X, y, batch_size=64, epochs=100)

# Save the model
model.save('shakespeare_rnn.h5')
# Load the pre-trained model (if available)
pretrained_model = tf.keras.models.load_model('shakespeare_rnn.h5')

def predict_text(seed_text, model, tokenizer, seq_length, num_chars_to_generate=100):
    # Ensure the seed_text is at least seq_length long by padding if necessary
    if len(seed_text) < seq_length:
        seed_text = seed_text.rjust(seq_length)  # Pad with spaces on the left if needed
    
    for _ in range(num_chars_to_generate):
        # Tokenize the last 'seq_length' characters from the seed_text
        token_list = tokenizer.texts_to_sequences([seed_text[-seq_length:]])[0]
        
        # Ensure token_list has the correct length (seq_length)
        token_list = np.pad(token_list, (seq_length - len(token_list), 0), 'constant')
        
        # Reshape for model input
        token_list = np.reshape(token_list, (1, seq_length))
        
        # Predict the next character
        predicted = np.argmax(model.predict(token_list, verbose=0), axis=-1)
        
        # Convert the predicted index back to the corresponding character
        output_char = tokenizer.index_word[predicted[0]]
        
        # Append the predicted character to the seed_text
        seed_text += output_char
        
    return seed_text


# Example of generating new text
seed_text = "To be, or not to be, that is the question:\n"
generated_text = predict_text(seed_text, pretrained_model, tokenizer, seq_length, num_chars_to_generate=100)
print(generated_text)





DLassi1.ipynb_
Notebook unstarred

1) Write a python program to implement and plot the given below Activation Functions of neural networks. a) Linear b) Sigmoid c) Tanh d) ReLU e) Softmax
import numpy as np
import matplotlib.pyplot as plt

# Linear activation functions
def linear(x):
    return x

# Sigmoid activation functions
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# Tanh activation functions
def tanh(x):
    return np.tanh(x)

# ReLU activation functions
def relu(x):
    return np.maximum(0, x)

# Softmax activation functions
def softmax(x):
    exp_x = np.exp(x - np.max(x))  # Stability improvement by subtracting max
    return exp_x / exp_x.sum(axis=0)

# range of input values
x = np.linspace(-10, 10, 100)

# Plot each activation function
plt.figure(figsize=(12, 8))

# Linear Activation Function return: f(x)=x
plt.subplot(2, 3, 1)
plt.plot(x, linear(x), label='Linear', color='b')
plt.title('Linear Activation')
plt.xlabel('x')
plt.ylabel('f(x)')
plt.grid()

# Sigmoid
plt.subplot(2, 3, 2)
plt.plot(x, sigmoid(x), label='Sigmoid', color='g')
plt.title('Sigmoid Activation')
plt.xlabel('x')
plt.ylabel('f(x)')
plt.grid()

# Tanh
plt.subplot(2, 3, 3)
plt.plot(x, tanh(x), label='Tanh', color='r')
plt.title('Tanh Activation')
plt.xlabel('x')
plt.ylabel('f(x)')
plt.grid()

# ReLU
plt.subplot(2, 3, 4)
plt.plot(x, relu(x), label='ReLU', color='m')
plt.title('ReLU Activation')
plt.xlabel('x')
plt.ylabel('f(x)')
plt.grid()

# Softmax
x_for_softmax = np.linspace(-5, 5, 10)
plt.subplot(2, 3, 5)
plt.plot(x_for_softmax, softmax(x_for_softmax), label='Softmax', color='c')
plt.title('Softmax Activation')
plt.xlabel('x')
plt.ylabel('f(x)')
plt.grid()

plt.tight_layout()
plt.show()

2]Implement a Single layer perceptron using a tensorflow library and calculate the accuracy of the model on the testing data. (Use MNIST dataset)

import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.utils import to_categorical

#Load the MNIST dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()

#Preprocess the data and Normalize the pixel values to [0, 1]
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0

# Convert labels to one-hot encoding
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

#Build the Single-Layer Perceptron model
model = Sequential([
    Flatten(input_shape=(28, 28)),       # Flatten the 28x28 images into a 1D array
    Dense(10, activation='softmax')      # Single dense layer with 10 output units (for 10 digits)
])

# Step 4: Compile the model
model.compile(optimizer='sgd',           # Stochastic Gradient Descent optimizer
              loss='categorical_crossentropy',  # Loss function for multi-class classification
              metrics=['accuracy'])      # Evaluate the model using accuracy

#Train the model
model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))

#Test the model on test data
test_loss, test_acc = model.evaluate(x_test, y_test)

print(f'Test accuracy: {test_acc:.4f}')

Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz
11490434/11490434 ━━━━━━━━━━━━━━━━━━━━ 0s 0us/step

/usr/local/lib/python3.10/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)

Epoch 1/10
1875/1875 ━━━━━━━━━━━━━━━━━━━━ 4s 2ms/step - accuracy: 0.7269 - loss: 1.1179 - val_accuracy: 0.8819 - val_loss: 0.4783
Epoch 2/10
1875/1875 ━━━━━━━━━━━━━━━━━━━━ 7s 3ms/step - accuracy: 0.8794 - loss: 0.4710 - val_accuracy: 0.8950 - val_loss: 0.3990
Epoch 3/10
1875/1875 ━━━━━━━━━━━━━━━━━━━━ 3s 2ms/step - accuracy: 0.8878 - loss: 0.4160 - val_accuracy: 0.9008 - val_loss: 0.3669
Epoch 4/10
1875/1875 ━━━━━━━━━━━━━━━━━━━━ 3s 2ms/step - accuracy: 0.8955 - loss: 0.3831 - val_accuracy: 0.9073 - val_loss: 0.3473
Epoch 5/10
1875/1875 ━━━━━━━━━━━━━━━━━━━━ 3s 2ms/step - accuracy: 0.8997 - loss: 0.3655 - val_accuracy: 0.9098 - val_loss: 0.3358
Epoch 6/10
1875/1875 ━━━━━━━━━━━━━━━━━━━━ 4s 2ms/step - accuracy: 0.9034 - loss: 0.3486 - val_accuracy: 0.9115 - val_loss: 0.3258
Epoch 7/10
1875/1875 ━━━━━━━━━━━━━━━━━━━━ 3s 2ms/step - accuracy: 0.9029 - loss: 0.3454 - val_accuracy: 0.9126 - val_loss: 0.3199
Epoch 8/10
1875/1875 ━━━━━━━━━━━━━━━━━━━━ 6s 2ms/step - accuracy: 0.9076 - loss: 0.3345 - val_accuracy: 0.9135 - val_loss: 0.3143
Epoch 9/10
1875/1875 ━━━━━━━━━━━━━━━━━━━━ 5s 2ms/step - accuracy: 0.9086 - loss: 0.3318 - val_accuracy: 0.9156 - val_loss: 0.3092
Epoch 10/10
1875/1875 ━━━━━━━━━━━━━━━━━━━━ 3s 2ms/step - accuracy: 0.9105 - loss: 0.3249 - val_accuracy: 0.9163 - val_loss: 0.3059
313/313 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - accuracy: 0.9031 - loss: 0.3504
Test accuracy: 0.9163

Write a python program to implement OR gate using a single layer perceptron learning rule.

import numpy as np

# Define the OR gate dataset (inputs and expected outputs)
X = np.array([[0, 0],
              [0, 1],
              [1, 0],
              [1, 1]])

y = np.array([0, 1, 1, 1])  #output

# Initialize weights and bias randomly
weights = np.random.rand(2)
bias = np.random.rand(1)

# Define the activation function (step function)
def step_function(net_input):
    return np.where(net_input >= 0, 1, 0)

# Define the perceptron learning rule
learning_rate = 0.1
max_epochs = 100  # Maximum number of iterations

def perceptron_train(X, y, weights, bias, learning_rate, max_epochs):
    for epoch in range(max_epochs):
        total_error = 0
        print(f"\nEpoch {epoch + 1}")

        for i in range(len(X)):
            #Calculate net input (weighted sum + bias)
            net_input = np.dot(X[i], weights) + bias

            #Apply step function to get output
            y_pred = step_function(net_input)

            #Calculate error (target - predicted)
            error = y[i] - y_pred

            #Update weights and bias based on the error
            weights += learning_rate * error * X[i]
            bias += learning_rate * error

            print(f"Input: {X[i]}, Predicted: {y_pred}, Expected: {y[i]}, Error: {error}")
            total_error += abs(error)

        # Stop training if there is no error
        if total_error == 0:
            print("Training complete after", epoch + 1, "epochs")
            break

    return weights, bias

# Step 5: Train the perceptron
weights, bias = perceptron_train(X, y, weights, bias, learning_rate, max_epochs)

# Step 6: Test the perceptron
def perceptron_predict(X, weights, bias):
    net_input = np.dot(X, weights) + bias
    return step_function(net_input)

# Predict the outputs for all inputs of the OR gate
predictions = perceptron_predict(X, weights, bias)

print("\nFinal weights:", weights)
print("Final bias:", bias)
print("Predictions for OR gate:", predictions)


Epoch 1
Input: [0 0], Predicted: [1], Expected: 0, Error: [-1]
Input: [0 1], Predicted: [1], Expected: 1, Error: [0]
Input: [1 0], Predicted: [1], Expected: 1, Error: [0]
Input: [1 1], Predicted: [1], Expected: 1, Error: [0]

Epoch 2
Input: [0 0], Predicted: [1], Expected: 0, Error: [-1]
Input: [0 1], Predicted: [1], Expected: 1, Error: [0]
Input: [1 0], Predicted: [1], Expected: 1, Error: [0]
Input: [1 1], Predicted: [1], Expected: 1, Error: [0]

Epoch 3
Input: [0 0], Predicted: [1], Expected: 0, Error: [-1]
Input: [0 1], Predicted: [1], Expected: 1, Error: [0]
Input: [1 0], Predicted: [1], Expected: 1, Error: [0]
Input: [1 1], Predicted: [1], Expected: 1, Error: [0]

Epoch 4
Input: [0 0], Predicted: [1], Expected: 0, Error: [-1]
Input: [0 1], Predicted: [1], Expected: 1, Error: [0]
Input: [1 0], Predicted: [1], Expected: 1, Error: [0]
Input: [1 1], Predicted: [1], Expected: 1, Error: [0]

Epoch 5
Input: [0 0], Predicted: [1], Expected: 0, Error: [-1]
Input: [0 1], Predicted: [1], Expected: 1, Error: [0]
Input: [1 0], Predicted: [1], Expected: 1, Error: [0]
Input: [1 1], Predicted: [1], Expected: 1, Error: [0]

Epoch 6
Input: [0 0], Predicted: [1], Expected: 0, Error: [-1]
Input: [0 1], Predicted: [1], Expected: 1, Error: [0]
Input: [1 0], Predicted: [1], Expected: 1, Error: [0]
Input: [1 1], Predicted: [1], Expected: 1, Error: [0]

Epoch 7
Input: [0 0], Predicted: [1], Expected: 0, Error: [-1]
Input: [0 1], Predicted: [1], Expected: 1, Error: [0]
Input: [1 0], Predicted: [1], Expected: 1, Error: [0]
Input: [1 1], Predicted: [1], Expected: 1, Error: [0]

Epoch 8
Input: [0 0], Predicted: [1], Expected: 0, Error: [-1]
Input: [0 1], Predicted: [1], Expected: 1, Error: [0]
Input: [1 0], Predicted: [1], Expected: 1, Error: [0]
Input: [1 1], Predicted: [1], Expected: 1, Error: [0]

Epoch 9
Input: [0 0], Predicted: [0], Expected: 0, Error: [0]
Input: [0 1], Predicted: [1], Expected: 1, Error: [0]
Input: [1 0], Predicted: [1], Expected: 1, Error: [0]
Input: [1 1], Predicted: [1], Expected: 1, Error: [0]
Training complete after 9 epochs

Final weights: [0.70241253 0.66346602]
Final bias: [-0.00903188]
Predictions for OR gate: [0 1 1 1]

Write a python program to implement AND gate using a single layer perceptron learning rule.

import numpy as np

#Define the AND gate dataset (inputs and expected outputs)
# Inputs (X)
X = np.array([[0, 0],
              [0, 1],
              [1, 0],
              [1, 1]])

y = np.array([0, 0, 0, 1])  #output

#Initialize weights and bias randomly
weights = np.random.rand(2)  # Two weights for two inputs
bias = np.random.rand(1)     # One bias

#Define the activation function (step function)
def step_function(net_input):
    return np.where(net_input >= 0, 1, 0)

#Define the perceptron learning rule
learning_rate = 0.1
max_epochs = 100

def perceptron_train(X, y, weights, bias, learning_rate, max_epochs):
    for epoch in range(max_epochs):
        total_error = 0
        print(f"\nEpoch {epoch + 1}")

        for i in range(len(X)):
            # Calculate net input (weighted sum + bias)
            net_input = np.dot(X[i], weights) + bias

            # Apply step function to get output
            y_pred = step_function(net_input)

            # Calculate error (target - predicted)
            error = y[i] - y_pred

            # Update weights and bias based on the error
            weights += learning_rate * error * X[i]
            bias += learning_rate * error

            print(f"Input: {X[i]}, Predicted: {y_pred}, Expected: {y[i]}, Error: {error}")
            total_error += abs(error)

        # Stop training if there is no error
        if total_error == 0:
            print("Training complete after", epoch + 1, "epochs")
            break

    return weights, bias

#Train the perceptron
weights, bias = perceptron_train(X, y, weights, bias, learning_rate, max_epochs)

#Test the perceptron
def perceptron_predict(X, weights, bias):
    net_input = np.dot(X, weights) + bias
    return step_function(net_input)

# Predict the outputs for all inputs of the AND gate
predictions = perceptron_predict(X, weights, bias)

print("\nFinal weights:", weights)
print("Final bias:", bias)
print("Predictions for AND gate:", predictions)


Epoch 1
Input: [0 0], Predicted: [1], Expected: 0, Error: [-1]
Input: [0 1], Predicted: [1], Expected: 0, Error: [-1]
Input: [1 0], Predicted: [1], Expected: 0, Error: [-1]
Input: [1 1], Predicted: [1], Expected: 1, Error: [0]

Epoch 2
Input: [0 0], Predicted: [0], Expected: 0, Error: [0]
Input: [0 1], Predicted: [0], Expected: 0, Error: [0]
Input: [1 0], Predicted: [1], Expected: 0, Error: [-1]
Input: [1 1], Predicted: [1], Expected: 1, Error: [0]

Epoch 3
Input: [0 0], Predicted: [0], Expected: 0, Error: [0]
Input: [0 1], Predicted: [0], Expected: 0, Error: [0]
Input: [1 0], Predicted: [1], Expected: 0, Error: [-1]
Input: [1 1], Predicted: [1], Expected: 1, Error: [0]

Epoch 4
Input: [0 0], Predicted: [0], Expected: 0, Error: [0]
Input: [0 1], Predicted: [0], Expected: 0, Error: [0]
Input: [1 0], Predicted: [1], Expected: 0, Error: [-1]
Input: [1 1], Predicted: [1], Expected: 1, Error: [0]

Epoch 5
Input: [0 0], Predicted: [0], Expected: 0, Error: [0]
Input: [0 1], Predicted: [0], Expected: 0, Error: [0]
Input: [1 0], Predicted: [1], Expected: 0, Error: [-1]
Input: [1 1], Predicted: [0], Expected: 1, Error: [1]

Epoch 6
Input: [0 0], Predicted: [0], Expected: 0, Error: [0]
Input: [0 1], Predicted: [0], Expected: 0, Error: [0]
Input: [1 0], Predicted: [1], Expected: 0, Error: [-1]
Input: [1 1], Predicted: [0], Expected: 1, Error: [1]

Epoch 7
Input: [0 0], Predicted: [0], Expected: 0, Error: [0]
Input: [0 1], Predicted: [0], Expected: 0, Error: [0]
Input: [1 0], Predicted: [1], Expected: 0, Error: [-1]
Input: [1 1], Predicted: [1], Expected: 1, Error: [0]

Epoch 8
Input: [0 0], Predicted: [0], Expected: 0, Error: [0]
Input: [0 1], Predicted: [0], Expected: 0, Error: [0]
Input: [1 0], Predicted: [0], Expected: 0, Error: [0]
Input: [1 1], Predicted: [1], Expected: 1, Error: [0]
Training complete after 8 epochs

Final weights: [0.28825468 0.15540616]
Final bias: [-0.42373533]
Predictions for AND gate: [0 0 0 1]

1) Write a python program to implement and plot the given below Activation Functions of neural networks. a) Hard Tanh b) Leaky ReLU c) ELU

import numpy as np
import matplotlib.pyplot as plt

#Define the activation functions

# a) Hard Tanh Activation Function
def hard_tanh(x):
    return np.where(x > 1, 1, np.where(x < -1, -1, x))

# b) Leaky ReLU Activation Function
def leaky_relu(x, alpha=0.01):
    return np.where(x > 0, x, alpha * x)

# c) ELU Activation Function
def elu(x, alpha=1.0):
    return np.where(x > 0, x, alpha * (np.exp(x) - 1))

#Generate input data
x = np.linspace(-3, 3, 400)

#Compute the outputs for each activation function
y_hard_tanh = hard_tanh(x)
y_leaky_relu = leaky_relu(x)
y_elu = elu(x)

#Plot the activation functions
plt.figure(figsize=(10, 6))

# Plot for Hard Tanh
plt.subplot(1, 3, 1)
plt.plot(x, y_hard_tanh, label='Hard Tanh', color='blue')
plt.title('Hard Tanh Activation')
plt.grid(True)
plt.xlabel('Input')
plt.ylabel('Output')

# Plot for Leaky ReLU
plt.subplot(1, 3, 2)
plt.plot(x, y_leaky_relu, label='Leaky ReLU', color='green')
plt.title('Leaky ReLU Activation')
plt.grid(True)
plt.xlabel('Input')

# Plot for ELU
plt.subplot(1, 3, 3)
plt.plot(x, y_elu, label='ELU', color='red')
plt.title('ELU Activation')
plt.grid(True)
plt.xlabel('Input')

# Show the plots
plt.tight_layout()
plt.show()

Write a python program to implement a NAND gate using a single layer perceptron learning rule.

import numpy as np

# Define the NAND gate dataset (inputs and expected outputs)
# Inputs (X)
X = np.array([[0, 0],
              [0, 1],
              [1, 0],
              [1, 1]])

y = np.array([1, 1, 1, 0])  # output

#Initialize weights and bias randomly
weights = np.random.rand(2)
bias = np.random.rand(1)

# activation function
def step_function(net_input):
    return np.where(net_input >= 0, 1, 0)

# perceptron learning rule
learning_rate = 0.1
max_epochs = 100

def perceptron_train(X, y, weights, bias, learning_rate, max_epochs):
    for epoch in range(max_epochs):
        total_error = 0
        print(f"\nEpoch {epoch + 1}")

        for i in range(len(X)):
            #Calculate net input (weighted sum + bias)
            net_input = np.dot(X[i], weights) + bias

            #Apply step function to get output
            y_pred = step_function(net_input)

            #Calculate error (target - predicted)
            error = y[i] - y_pred

            #Update weights and bias based on the error
            weights += learning_rate * error * X[i]
            bias += learning_rate * error

            print(f"Input: {X[i]}, Predicted: {y_pred}, Expected: {y[i]}, Error: {error}")
            total_error += abs(error)

        # Stop training if there is no error
        if total_error == 0:
            print("Training complete after", epoch + 1, "epochs")
            break

    return weights, bias

# Train the perceptron
weights, bias = perceptron_train(X, y, weights, bias, learning_rate, max_epochs)

# Test the perceptron
def perceptron_predict(X, weights, bias):
    net_input = np.dot(X, weights) + bias
    return step_function(net_input)

# Predict the outputs for all inputs of the NAND gate
predictions = perceptron_predict(X, weights, bias)

print("\nFinal weights:", weights)
print("Final bias:", bias)
print("Predictions for NAND gate:", predictions)


Epoch 1
Input: [0 0], Predicted: [1], Expected: 1, Error: [0]
Input: [0 1], Predicted: [1], Expected: 1, Error: [0]
Input: [1 0], Predicted: [1], Expected: 1, Error: [0]
Input: [1 1], Predicted: [1], Expected: 0, Error: [-1]

Epoch 2
Input: [0 0], Predicted: [1], Expected: 1, Error: [0]
Input: [0 1], Predicted: [1], Expected: 1, Error: [0]
Input: [1 0], Predicted: [1], Expected: 1, Error: [0]
Input: [1 1], Predicted: [1], Expected: 0, Error: [-1]

Epoch 3
Input: [0 0], Predicted: [1], Expected: 1, Error: [0]
Input: [0 1], Predicted: [1], Expected: 1, Error: [0]
Input: [1 0], Predicted: [1], Expected: 1, Error: [0]
Input: [1 1], Predicted: [1], Expected: 0, Error: [-1]

Epoch 4
Input: [0 0], Predicted: [1], Expected: 1, Error: [0]
Input: [0 1], Predicted: [1], Expected: 1, Error: [0]
Input: [1 0], Predicted: [1], Expected: 1, Error: [0]
Input: [1 1], Predicted: [1], Expected: 0, Error: [-1]

Epoch 5
Input: [0 0], Predicted: [1], Expected: 1, Error: [0]
Input: [0 1], Predicted: [1], Expected: 1, Error: [0]
Input: [1 0], Predicted: [1], Expected: 1, Error: [0]
Input: [1 1], Predicted: [1], Expected: 0, Error: [-1]

Epoch 6
Input: [0 0], Predicted: [1], Expected: 1, Error: [0]
Input: [0 1], Predicted: [1], Expected: 1, Error: [0]
Input: [1 0], Predicted: [1], Expected: 1, Error: [0]
Input: [1 1], Predicted: [0], Expected: 0, Error: [0]
Training complete after 6 epochs

Final weights: [-0.10784899 -0.17304881]
Final bias: [0.26074505]
Predictions for NAND gate: [1 1 1 0]

Colab paid products - Cancel contracts here
